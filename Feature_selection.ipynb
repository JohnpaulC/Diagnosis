{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Feature selection based on the importance of XGBoost Model\n",
    "\n",
    "Re-train the model and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T12:32:04.178872Z",
     "start_time": "2020-08-03T12:32:04.020091Z"
    }
   },
   "outputs": [],
   "source": [
    "select_col = [38, 39, 43, 44]\n",
    "\n",
    "dataset = np.load('data/satellite_state.npy')\n",
    "X = dataset[:, :-1]\n",
    "y = dataset[-1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dataset[:, :-1], dataset[:, -1], random_state=1)\n",
    "x_train = x_train[:, select_col]\n",
    "x_test = x_test[:, select_col]\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "since = time.time()\n",
    "num_round = 200\n",
    "\n",
    "param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\n",
    "xgb_model = xgb.train(param, dtrain, num_round, evallist)\n",
    "#xgb_model.fit(x_train, y_train)\n",
    "\n",
    "xgb.plot_tree(xgb_model, num_trees=0)\n",
    "xgb.plot_importance(xgb_model)\n",
    "plt.show()\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.2f}s'.format(time_elapsed))\n",
    "predictions = xgb_model.predict(dtest)\n",
    "predictions = np.array(predictions > 0.5)\n",
    "\n",
    "cm_perf = (confusion_matrix(y_test, predictions))\n",
    "print(\"Confusion matrix: \\n\", cm_perf)\n",
    "perf = perf_parse(cm_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different models\n",
    "\n",
    "Feature selection using different classification methods.\n",
    "\n",
    "- LR\n",
    "- SVM\n",
    "- Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T13:51:57.630470Z",
     "start_time": "2020-05-17T13:51:57.178843Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from test_xgboost import perf_parse\n",
    "    \n",
    "since = time.time()\n",
    "# Standard preprocess the training data\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train_s = scaler.transform(x_train)\n",
    "x_test_s = scaler.transform(x_test)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=20000)\n",
    "lr_model.fit(x_train_s, y_train)\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.2f}s'.format(time_elapsed))\n",
    "\n",
    "predictions = lr_model.predict(x_train_s)\n",
    "cm_perf = (confusion_matrix(y_train, predictions))\n",
    "print(\"Confusion matrix: \\n\", cm_perf)\n",
    "perf = perf_parse(cm_perf)\n",
    "\n",
    "print(\"LR train score: {0:.3f}\".format(lr_model.score(x_train_s, y_train)))\n",
    "print(\"LR test score: {0:.3f}\".format(lr_model.score(x_test_s, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T13:55:10.892102Z",
     "start_time": "2020-05-17T13:53:25.791117Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from test_xgboost import perf_parse\n",
    "\n",
    "\n",
    "since = time.time()\n",
    "# Standard preprocess the training data\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train_s = scaler.transform(x_train)\n",
    "x_test_s = scaler.transform(x_test)\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(x_train_s, y_train)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.2f}s'.format(time_elapsed))\n",
    "\n",
    "predictions = svm_model.predict(x_train_s)\n",
    "cm_perf = (confusion_matrix(y_train, predictions))\n",
    "print(\"Confusion matrix: \\n\", cm_perf)\n",
    "perf = perf_parse(cm_perf)\n",
    "\n",
    "print(\"SVM train score: {0:.3f}\".format(svm_model.score(x_train_s, y_train)))\n",
    "print(\"SVM test score: {0:.3f}\".format(svm_model.score(x_test_s, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:15:07.620700Z",
     "start_time": "2020-05-17T15:12:14.549505Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Accuracy\n",
    "def get_accuracy(y_pred, y_target):\n",
    "    n_correct = torch.eq(y_pred, y_target).sum().item()\n",
    "    accuracy = n_correct / len(y_pred) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Multilayer Perceptron\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.fc3 = nn.Linear(hidden_size[1], num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        a_1 = F.relu(self.fc1(x_in))\n",
    "        a_2 = F.relu(self.fc2(a_1))\n",
    "        y_pred = self.fc3(a_2)\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Standard preprocess the training data\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "score = 100\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# Model configuration\n",
    "[m_train, n_train] = x_train.shape\n",
    "input_size = n_train\n",
    "hidden_size = [60, 30]\n",
    "num_classes = 2\n",
    "# Train configuration\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "dropout_p = 0.5\n",
    "step_size = 500\n",
    "\n",
    "model = MLP(input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_classes=num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.5)\n",
    "\n",
    "since = time.time()\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "# Training\n",
    "for t in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)\n",
    "\n",
    "    # Accuracy\n",
    "    _, predictions = y_pred.max(dim=1)\n",
    "    accuracy = get_accuracy(y_pred=predictions.long(), y_target=y_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        _, pred_test = model(x_test, apply_softmax=True).max(dim=1)\n",
    "        test_acc = get_accuracy(y_pred=pred_test, y_target=y_test)\n",
    "        # deep copy the model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if t % 500 == 0:\n",
    "            print(\"epoch: {0:4d} | loss: {1:.4f} | Train accuracy: {2:.1f}% | Test accuracy: {3:.1f}%\"\n",
    "                  .format(t, loss, accuracy, test_acc))\n",
    "\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.2f}s'.format(time_elapsed))\n",
    "#print('Best val Acc: {:4f}'.format(best_acc))\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Predictions\n",
    "_, pred_train = model(x_train, apply_softmax=True).max(dim=1)\n",
    "_, pred_test = model(x_test, apply_softmax=True).max(dim=1)\n",
    "\n",
    "# Train and test accuracies\n",
    "train_acc = get_accuracy(y_pred=pred_train, y_target=y_train)\n",
    "test_acc = get_accuracy(y_pred=pred_test, y_target=y_test)\n",
    "print(\"train acc: {0:.1f}%, test acc: {1:.1f}%\".format(\n",
    "    train_acc, test_acc))\n",
    "\n",
    "y_true = y_test.cpu().numpy()\n",
    "y_pred = pred_test.cpu().numpy()\n",
    "\n",
    "cm_perf = confusion_matrix(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(cm_perf)\n",
    "acc = (cm_perf[1, 1] + cm_perf[0, 0]) / np.sum(cm_perf)\n",
    "recall = cm_perf[1, 1] / (cm_perf[1, 0] + cm_perf[1, 1])\n",
    "precision = cm_perf[1, 1] / (cm_perf[0, 1] + cm_perf[1, 1])\n",
    "score = 2 / ((1 / recall) + (1 / precision))\n",
    "model_perf = torch.tensor([acc, precision, recall, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
